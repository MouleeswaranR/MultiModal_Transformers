{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LHPzLUXP1N8l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#each row corresponds to each token\n",
        "inputs=torch.tensor(\n",
        "    [\n",
        "        [0.72,0.45,0.31],\n",
        "        [0.75,0.20,0.55],\n",
        "        [0.30,0.80,0.40],\n",
        "        [0.85,0.35,0.60],\n",
        "        [0.55,0.15,0.75],\n",
        "        [0.25,0.20,0.85]\n",
        "    ]\n",
        ")\n",
        "\n",
        "words=['Dream','big','and','work','for','it']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using 2nd token \"big\"\n",
        "x_2=inputs[1]\n",
        "\n",
        "#d_in,d_out are dimensions for wq,wk,wv matrices\n",
        "d_in=inputs.shape[1]\n",
        "d_out=2\n",
        "print(x_2)\n",
        "print(d_in)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUOUFJJuMG2J",
        "outputId": "322b48f7-d68f-4ee0-8849-2d8c7f093711"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7500, 0.2000, 0.5500])\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#randomly initializing wq,wk,wv matrices\n",
        "\n",
        "torch.manual_seed(123)\n",
        "W_query=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "W_key=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "W_value=torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)"
      ],
      "metadata": {
        "id": "sf1J93hUMZib"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(W_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r91w6ibDNg2L",
        "outputId": "f42e2934-fc98-49b0-8bfa-02aeb33572a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_2=x_2@W_query\n",
        "#query for 2nd token(input embedding for 2nd token * query matrix)\n",
        "key_2=x_2@W_key\n",
        "#key for 2nd token(input embedding for 2nd token * key matrix)\n",
        "value_2=x_2@W_value\n",
        "#value for 2nd token(input embedding for 2nd token * value matrix)\n"
      ],
      "metadata": {
        "id": "F6kARJSSNpEv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating key,query,value for all tokens\n",
        "\n",
        "keys=inputs @ W_key\n",
        "queries=inputs @ W_query\n",
        "values=inputs @ W_value\n",
        "\n",
        "print(\"keys shape: \",keys.shape)\n",
        "print(\"queries shape: \",queries.shape)\n",
        "print(\"values shape: \",values.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUx_kURAOKMt",
        "outputId": "e9f50103-c400-4c4d-808a-f9bf96b0512a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys shape:  torch.Size([6, 2])\n",
            "queries shape:  torch.Size([6, 2])\n",
            "values shape:  torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating attention score for 2nd token with respect to 2nd token\n",
        "keys_2=keys[1]#key for 2nd token\n",
        "attn_score22=query_2.dot(keys_2)\n",
        "print(attn_score22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrFm9kjwOway",
        "outputId": "8cab3c8c-2d71-4751-a231-ba15e6d84cf0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6990)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating attention score for 2nd token with respect to all tokens\n",
        "attn_scores2=query_2@keys.T\n",
        "print(attn_scores2)\n",
        "\n",
        "d_k=keys.shape[-1]\n",
        "\n",
        "#scaling down the attention weights and applying softmax\n",
        "attn_weights2=torch.softmax(attn_scores2/d_k**0.5,dim=-1)\n",
        "print(attn_weights2)\n",
        "\n",
        "#context vector for 2nd token\n",
        "context_vec2=attn_weights2@ values\n",
        "print(context_vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tps-0X7FPTeI",
        "outputId": "96896a32-d4f2-449d-abf3-57d6cfa619b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7021, 0.6990, 0.9867, 0.8707, 0.7880, 0.8624])\n",
            "tensor([0.1531, 0.1528, 0.1873, 0.1725, 0.1627, 0.1715])\n",
            "tensor([0.2274, 0.7362])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating attention score for all tokens with respect to 2nd token\n",
        "attn_scores=queries @ keys.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8q3TAlmPyzR",
        "outputId": "097ac336-a683-46bd-d15b-cdba4b7a6a3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6807, 0.6795, 0.9526, 0.8454, 0.7654, 0.8359],\n",
            "        [0.7021, 0.6990, 0.9867, 0.8707, 0.7880, 0.8624],\n",
            "        [0.7350, 0.7315, 1.0337, 0.9113, 0.8248, 0.9029],\n",
            "        [0.8436, 0.8402, 1.1848, 1.0464, 0.9471, 1.0361],\n",
            "        [0.7080, 0.7025, 1.0003, 0.8764, 0.7929, 0.8699],\n",
            "        [0.6680, 0.6606, 0.9486, 0.8254, 0.7465, 0.8210]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling down by square root of dimension of keys to reduce variance to 1 and applying softmax\n",
        "d_k=keys.shape[-1]\n",
        "attn_weights=torch.softmax(attn_scores/d_k**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN1LBvsOQBSb",
        "outputId": "8f88302c-5441-4666-9e77-a1b6a4bb1435"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1536, 0.1534, 0.1861, 0.1725, 0.1630, 0.1714],\n",
            "        [0.1531, 0.1528, 0.1873, 0.1725, 0.1627, 0.1715],\n",
            "        [0.1525, 0.1521, 0.1884, 0.1728, 0.1625, 0.1717],\n",
            "        [0.1505, 0.1501, 0.1915, 0.1737, 0.1619, 0.1724],\n",
            "        [0.1530, 0.1524, 0.1881, 0.1724, 0.1625, 0.1716],\n",
            "        [0.1538, 0.1530, 0.1875, 0.1719, 0.1625, 0.1713]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec=attn_weights @ values\n",
        "print(context_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXu1wtJRQdCc",
        "outputId": "d257fa98-4804-41cc-b371-cc64e906c3e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2273, 0.7361],\n",
            "        [0.2274, 0.7362],\n",
            "        [0.2276, 0.7363],\n",
            "        [0.2280, 0.7368],\n",
            "        [0.2275, 0.7362],\n",
            "        [0.2275, 0.7360]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Class for Self Attention\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    W_query=nn.Parameter(torch.randn(d_in,d_out))\n",
        "    W_key=nn.Parameter(torch.randn(d_in,d_out))\n",
        "    W_value=nn.Parameter(torch.randn(d_in,d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys=x @ self.W_key\n",
        "    queries=x @ self.W_query\n",
        "    values=x @ self.W_value\n",
        "\n",
        "    attn_scores=queries@keys.T\n",
        "\n",
        "    attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "\n",
        "    context_vecs=attn_weights @ values\n",
        "\n",
        "    return context_vecs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xw2GsYCdRFdS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (seq_len, d_in) or (batch, seq_len, d_in)\n",
        "\n",
        "        queries = self.W_query(x)\n",
        "        keys    = self.W_key(x)\n",
        "        values  = self.W_value(x)\n",
        "\n",
        "        d_k = keys.size(-1)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(-2, -1)\n",
        "        attn_weights = torch.softmax(attn_scores / d_k**0.5, dim=-1)\n",
        "\n",
        "        context_vecs = attn_weights @ values\n",
        "        return context_vecs"
      ],
      "metadata": {
        "id": "-V4lJieGS3gg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "d_in=inputs.shape[-1]\n",
        "d_out=2\n",
        "\n",
        "sa_v2=SelfAttention_v2(d_in,d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "id": "5xF0xEykTRqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bed0d7d-3d76-4475-9ca9-54d7c718a82a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5282, -0.0051],\n",
            "        [-0.5288, -0.0036],\n",
            "        [-0.5276, -0.0066],\n",
            "        [-0.5289, -0.0040],\n",
            "        [-0.5289, -0.0032],\n",
            "        [-0.5287, -0.0033]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "queries=sa_v2.W_query(inputs)\n",
        "keys=sa_v2.W_key(inputs)\n",
        "attn_scores=queries@keys.T\n",
        "attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUt2-uVT8AyL",
        "outputId": "25912f4f-051e-4d79-8d64-be3da288ab91"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1666, 0.1670, 0.1658, 0.1709, 0.1661, 0.1636],\n",
            "        [0.1656, 0.1690, 0.1606, 0.1743, 0.1677, 0.1628],\n",
            "        [0.1675, 0.1651, 0.1710, 0.1676, 0.1644, 0.1643],\n",
            "        [0.1658, 0.1687, 0.1614, 0.1746, 0.1673, 0.1623],\n",
            "        [0.1653, 0.1696, 0.1591, 0.1751, 0.1682, 0.1627],\n",
            "        [0.1655, 0.1692, 0.1601, 0.1740, 0.1680, 0.1632]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#applying casual attention\n",
        "\n",
        "# no of tokens is the context length\n",
        "context_length=attn_scores.shape[0]\n",
        "\n",
        "#mask with ones in lower diagonal to capture the lower diagonal values of attentiion weights\n",
        "mask_simple=torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZIwBAEd_E1J",
        "outputId": "b0f1b799-c61c-4f79-8960-f49b7aa634d2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_attn_weights=attn_weights*mask_simple\n",
        "print(masked_attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-xHIf4S_mnw",
        "outputId": "4cc82a66-9bd6-40e5-bafd-28e06ee8066c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1666, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1656, 0.1690, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1675, 0.1651, 0.1710, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1658, 0.1687, 0.1614, 0.1746, 0.0000, 0.0000],\n",
            "        [0.1653, 0.1696, 0.1591, 0.1751, 0.1682, 0.0000],\n",
            "        [0.1655, 0.1692, 0.1601, 0.1740, 0.1680, 0.1632]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize the maksed attention weights - Naive Approach\n",
        "#this leads to data leakage\n",
        "\n",
        "#finding row sum of each row\n",
        "row_sums=masked_attn_weights.sum(dim=1,keepdim=True)\n",
        "\n",
        "#dividing each row vaues lby its row sum\n",
        "masked_simple_norm=masked_attn_weights/row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tOE5x4S_wx0",
        "outputId": "b56485c7-11c7-408c-91aa-96bc81de4d16"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4949, 0.5051, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3326, 0.3278, 0.3395, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2472, 0.2516, 0.2407, 0.2604, 0.0000, 0.0000],\n",
            "        [0.1974, 0.2025, 0.1900, 0.2092, 0.2009, 0.0000],\n",
            "        [0.1655, 0.1692, 0.1601, 0.1740, 0.1680, 0.1632]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing the upper diagonal of attention scores of upper diagonal to negative inifinity\n",
        "\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIXDrskrAKsp",
        "outputId": "6f6a2770-7816-46bf-d7c8-2c7b0e39562f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1659, 0.1698, 0.1592, 0.2024, 0.1614, 0.1403],\n",
            "        [0.2529, 0.2817, 0.2094, 0.3258, 0.2710, 0.2289],\n",
            "        [0.0804, 0.0600, 0.1096, 0.0811, 0.0539, 0.0534],\n",
            "        [0.2697, 0.2946, 0.2320, 0.3430, 0.2826, 0.2403],\n",
            "        [0.2703, 0.3064, 0.2162, 0.3523, 0.2954, 0.2481],\n",
            "        [0.2361, 0.2674, 0.1891, 0.3075, 0.2578, 0.2166]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask=torch.triu(torch.ones(context_length,context_length),diagonal=1)\n",
        "#upper diagonal with 1\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIw2IiKqC7S_",
        "outputId": "204fb0a1-693a-44c8-b72c-6cc5ebc6e0fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#masked attention scores with negative infinity where mask has 1(it has 1 in upper diagonal)\n",
        "masked_inf=attn_scores.masked_fill(mask.bool(),-torch.inf)\n",
        "print(masked_inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8StJfosiDDke",
        "outputId": "4bff1759-4f98-44c3-bde3-8c4f5a0b5678"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1659,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.2529, 0.2817,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.0804, 0.0600, 0.1096,   -inf,   -inf,   -inf],\n",
            "        [0.2697, 0.2946, 0.2320, 0.3430,   -inf,   -inf],\n",
            "        [0.2703, 0.3064, 0.2162, 0.3523, 0.2954,   -inf],\n",
            "        [0.2361, 0.2674, 0.1891, 0.3075, 0.2578, 0.2166]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights=torch.softmax(masked_inf/keys.shape[-1]**0.5,dim=-1)\n",
        "#now automatically each row value sums up to 1 with softmax\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYp1kB6NDTEM",
        "outputId": "258a6e7b-7de8-404c-99e6-6cd234217c3f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4949, 0.5051, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3326, 0.3278, 0.3395, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2472, 0.2516, 0.2407, 0.2604, 0.0000, 0.0000],\n",
            "        [0.1974, 0.2025, 0.1900, 0.2092, 0.2009, 0.0000],\n",
            "        [0.1655, 0.1692, 0.1601, 0.1740, 0.1680, 0.1632]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dropout attention scores with 50%\n",
        "example=torch.ones(context_length,context_length)\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvRk-MzNDmAt",
        "outputId": "4ed8bfc5-0ca6-482b-96c6-f473b3d360ec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout=torch.nn.Dropout(0.5)\n",
        "# 1/1-p here p=0.5 1/0.5=2 scale up remaining weights by 2\n",
        "#50% of weights is dropped in each row and remaining are scaled up by 2%\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l62hVUXpGCcd",
        "outputId": "b24870d6-b5e0-4cdb-8069-0ef158a93cd3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_o8_mAT_GPD5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}