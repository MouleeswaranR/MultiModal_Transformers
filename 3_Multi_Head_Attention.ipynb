{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zJ5OPpDBPWKo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#each row corresponds to each token\n",
        "inputs=torch.tensor(\n",
        "    [\n",
        "        [0.72,0.45,0.31],\n",
        "        [0.75,0.20,0.55],\n",
        "        [0.30,0.80,0.40],\n",
        "        [0.85,0.35,0.60],\n",
        "        [0.55,0.15,0.75],\n",
        "        [0.25,0.20,0.85]\n",
        "    ]\n",
        ")\n",
        "\n",
        "words=['Dream','big','and','work','for','it']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CasualAttention(nn.Module):\n",
        "\n",
        "  def __init__(self,d_in,d_out,context_length,dropout=0.5,qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size,num_tokens,d_in=x.shape\n",
        "\n",
        "    keys=self.W_key(x)\n",
        "    queries=self.W_query(x)\n",
        "    values=self.W_value(x)\n",
        "\n",
        "    attn_scores=queries@keys.transpose(1,2)\n",
        "    attn_scores.masked_fill_(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf)\n",
        "    attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    context_vector=attn_weights @ values\n",
        "    return context_vector"
      ],
      "metadata": {
        "id": "QwKRC4sJPXn1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_in=inputs.shape[-1]\n",
        "d_out=2"
      ],
      "metadata": {
        "id": "iAfmMnO5Tixb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch=torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgKTRtoSUzKb",
        "outputId": "7a89fbbd-e891-49a4-a786-a9b0f160d259"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Multi-Head Attention without weight splits - inefficient as each head requires separate weight,query,value matrix multiplications\n",
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads=nn.ModuleList(\n",
        "        [CasualAttention(d_in,d_out,context_length,dropout,qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    #concatenating all context vectors from all heads along column\n",
        "    return torch.cat([head(x) for head in self.heads],dim=-1)"
      ],
      "metadata": {
        "id": "gkoVPstLU6l2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length=batch.shape[1]\n",
        "d_in,d_out=3,2\n",
        "mha=MultiHeadAttentionWrapper(d_in,d_out,context_length,0.0,num_heads=2)"
      ],
      "metadata": {
        "id": "-V5c6sUdV-fQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_vecs=mha(batch)\n",
        "print(context_vecs)\n",
        "print(context_vecs.shape)\n",
        "#here the shape last column 4 denotes that each input is 6x2 but because of 2 heads it is 2*2=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkDTBrTxWTua",
        "outputId": "7edae8ee-f471-4edd-e613-5a52ab113a9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.5762, -0.1627,  0.5569,  0.3635],\n",
            "         [-0.5650, -0.0630,  0.5599,  0.3006],\n",
            "         [-0.5472, -0.1226,  0.5285,  0.3435],\n",
            "         [-0.5787, -0.0943,  0.5621,  0.3388],\n",
            "         [-0.5593, -0.0436,  0.5509,  0.3046],\n",
            "         [-0.5287, -0.0033,  0.5277,  0.2743]],\n",
            "\n",
            "        [[-0.5762, -0.1627,  0.5569,  0.3635],\n",
            "         [-0.5650, -0.0630,  0.5599,  0.3006],\n",
            "         [-0.5472, -0.1226,  0.5285,  0.3435],\n",
            "         [-0.5787, -0.0943,  0.5621,  0.3388],\n",
            "         [-0.5593, -0.0436,  0.5509,  0.3046],\n",
            "         [-0.5287, -0.0033,  0.5277,  0.2743]]], grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.tensor([[\n",
        "    [1.0,2.0,3.0,4.0,5.0,6.0],\n",
        "    [6.0,5.0,4.0,3.0,2.0,1.0],\n",
        "    [1.0,1.0,1.0,1.0,1.0,1.0]\n",
        "]])\n",
        "\n",
        "batch_size,num_tokens,d_in=x.shape"
      ],
      "metadata": {
        "id": "8Ip1k9QPWwTd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random matrices for wq,wk,wv\n",
        "torch.manual_seed(0)\n",
        "\n",
        "Wq=torch.randn(d_in,d_in)\n",
        "Wk=torch.randn(d_in,d_in)\n",
        "Wv=torch.randn(d_in,d_in)\n",
        "\n",
        "key=x@Wk\n",
        "query=x@Wq\n",
        "value=x@Wv\n",
        "\n",
        "\n",
        "print(\"Query:\\n\",query)\n",
        "print(\"Key:\\n\",key)\n",
        "print(\"Value:\\n\",value)\n",
        "\n",
        "print(\"x shape:\",x.shape)\n",
        "print(\"Wq shape:\",Wq.shape)\n",
        "print(\"Wk shape:\",Wk.shape)\n",
        "print(\"Wv shape:\",Wv.shape)\n",
        "print(\"query shape:\",query.shape)\n",
        "print(\"key shape:\",key.shape)\n",
        "print(\"value shape:\",value.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An3w51sU7zac",
        "outputId": "b22ab740-e417-4142-90c1-37ad9745ac8b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            " tensor([[[ -9.0244, -11.7287,  15.5360,  -1.4474,  -4.5326,   9.4674],\n",
            "         [ -8.0564, -13.2309,   8.2228,  -8.9680,   3.1995,   4.8321],\n",
            "         [ -2.4401,  -3.5657,   3.3941,  -1.4879,  -0.1904,   2.0428]]])\n",
            "Key:\n",
            " tensor([[[  8.2602,  14.1116,  -5.0345, -16.4865,  -2.9948,   8.3139],\n",
            "         [ -6.1188,  -0.1587,  -5.0885, -14.3014,   4.9540,   5.6093],\n",
            "         [  0.3059,   1.9933,  -1.4461,  -4.3983,   0.2799,   1.9890]]])\n",
            "Value:\n",
            " tensor([[[ 0.5076, -3.4353,  1.8576,  2.8041,  8.9427, 13.1841],\n",
            "         [-1.9113, -3.6934,  1.8502,  1.7622,  1.6981,  3.0978],\n",
            "         [-0.2005, -1.0184,  0.5297,  0.6523,  1.5201,  2.3260]]])\n",
            "x shape: torch.Size([1, 3, 6])\n",
            "Wq shape: torch.Size([6, 6])\n",
            "Wk shape: torch.Size([6, 6])\n",
            "Wv shape: torch.Size([6, 6])\n",
            "query shape: torch.Size([1, 3, 6])\n",
            "key shape: torch.Size([1, 3, 6])\n",
            "value shape: torch.Size([1, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_heads=2\n",
        "head_dim=3\n",
        "\n",
        "query=query.view(1,3,num_heads,head_dim)\n",
        "key=key.view(1,3,num_heads,head_dim)\n",
        "value=value.view(1,3,num_heads,head_dim)\n",
        "\n",
        "#(batch_size,no_of-tokens,no_of_heads,head_dim)\n",
        "print(\"Query after unrolling:\",query)\n",
        "print(\"\\nQuery shape after unrolling:\",query.shape)\n",
        "print(\"Key after unrolling:\",key)\n",
        "print(\"\\nKey shape after unrolling:\",key.shape)\n",
        "print(\"Value after unrolling:\",value)\n",
        "print(\"\\nValue shape after unrolling:\",value.shape)\n",
        "#here the values are arranged for each token with 2 heads of 3 dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IadK0qhF_IlO",
        "outputId": "7cfc947c-64af-433e-8f8d-e82e1e16479e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query after unrolling: tensor([[[[ -9.0244, -11.7287,  15.5360],\n",
            "          [ -1.4474,  -4.5326,   9.4674]],\n",
            "\n",
            "         [[ -8.0564, -13.2309,   8.2228],\n",
            "          [ -8.9680,   3.1995,   4.8321]],\n",
            "\n",
            "         [[ -2.4401,  -3.5657,   3.3941],\n",
            "          [ -1.4879,  -0.1904,   2.0428]]]])\n",
            "\n",
            "Query shape after unrolling: torch.Size([1, 3, 2, 3])\n",
            "Key after unrolling: tensor([[[[  8.2602,  14.1116,  -5.0345],\n",
            "          [-16.4865,  -2.9948,   8.3139]],\n",
            "\n",
            "         [[ -6.1188,  -0.1587,  -5.0885],\n",
            "          [-14.3014,   4.9540,   5.6093]],\n",
            "\n",
            "         [[  0.3059,   1.9933,  -1.4461],\n",
            "          [ -4.3983,   0.2799,   1.9890]]]])\n",
            "\n",
            "Key shape after unrolling: torch.Size([1, 3, 2, 3])\n",
            "Value after unrolling: tensor([[[[ 0.5076, -3.4353,  1.8576],\n",
            "          [ 2.8041,  8.9427, 13.1841]],\n",
            "\n",
            "         [[-1.9113, -3.6934,  1.8502],\n",
            "          [ 1.7622,  1.6981,  3.0978]],\n",
            "\n",
            "         [[-0.2005, -1.0184,  0.5297],\n",
            "          [ 0.6523,  1.5201,  2.3260]]]])\n",
            "\n",
            "Value shape after unrolling: torch.Size([1, 3, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#grouping based on heads\n",
        "\n",
        "#(batch_size,no_of-tokens,no_of_heads,head_dim)\n",
        "#dim-1(no_of_tokens),dim-2(no_of_heads) before transposing\n",
        "query=query.transpose(1,2)\n",
        "value=value.transpose(1,2)\n",
        "key=key.transpose(1,2)\n",
        "\n",
        "#After transposing\n",
        "#(batch_size,no_of_heads,no_of-tokens,head_dim)\n",
        "\n",
        "print(\"Query after grouped by heads:\",query)\n",
        "print(\"\\nQuery shape after grouped by heads:\",query.shape)\n",
        "print(\"Key after grouped by heads:\",key)\n",
        "print(\"\\nKey shape after grouped by heads:\",key.shape)\n",
        "print(\"Value after grouped by heads:\",value)\n",
        "print(\"\\nValue shape after grouped by heads:\",value.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJb-fYRbAFzs",
        "outputId": "598eab3a-6c16-4598-f51d-39f86f43aa78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query after grouped by heads: tensor([[[[ -9.0244, -11.7287,  15.5360],\n",
            "          [ -8.0564, -13.2309,   8.2228],\n",
            "          [ -2.4401,  -3.5657,   3.3941]],\n",
            "\n",
            "         [[ -1.4474,  -4.5326,   9.4674],\n",
            "          [ -8.9680,   3.1995,   4.8321],\n",
            "          [ -1.4879,  -0.1904,   2.0428]]]])\n",
            "\n",
            "Query shape after grouped by heads: torch.Size([1, 2, 3, 3])\n",
            "Key after grouped by heads: tensor([[[[  8.2602,  14.1116,  -5.0345],\n",
            "          [ -6.1188,  -0.1587,  -5.0885],\n",
            "          [  0.3059,   1.9933,  -1.4461]],\n",
            "\n",
            "         [[-16.4865,  -2.9948,   8.3139],\n",
            "          [-14.3014,   4.9540,   5.6093],\n",
            "          [ -4.3983,   0.2799,   1.9890]]]])\n",
            "\n",
            "Key shape after grouped by heads: torch.Size([1, 2, 3, 3])\n",
            "Value after grouped by heads: tensor([[[[ 0.5076, -3.4353,  1.8576],\n",
            "          [-1.9113, -3.6934,  1.8502],\n",
            "          [-0.2005, -1.0184,  0.5297]],\n",
            "\n",
            "         [[ 2.8041,  8.9427, 13.1841],\n",
            "          [ 1.7622,  1.6981,  3.0978],\n",
            "          [ 0.6523,  1.5201,  2.3260]]]])\n",
            "\n",
            "Value shape after grouped by heads: torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding attention scores\n",
        "\n",
        "#transpose last 2 dimensions of key\n",
        "\n",
        "#before transpose:#(batch_size,no_of_heads,no_of-tokens,head_dim)\n",
        "key_t=key.transpose(2,3)\n",
        "#after transpose:#(batch_size,no_of_heads,head_dim,no_of-tokens)\n",
        "\n",
        "attn_scores=query@key_t\n",
        "print(\"Attention scores:\",attn_scores)\n",
        "#(batch_size,no_of_heads,no_of-tokens,no_of-tokens)- tells how much each token related with all tokens for each head\n",
        "print(\"Attention scores shape:\",attn_scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKBelyr3CQbg",
        "outputId": "6eadcf93-3048-4968-c6b4-f3936e06273b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores: tensor([[[[-318.2692,  -21.9748,  -48.6063],\n",
            "          [-294.6535,    9.5538,  -40.7285],\n",
            "          [ -87.5604,   -1.7744,  -12.7621]],\n",
            "\n",
            "         [[ 116.1476,   51.3506,   23.9283],\n",
            "          [ 178.4425,  171.2106,   49.9505],\n",
            "          [  42.0843,   31.7945,   10.5541]]]])\n",
            "Attention scores shape: torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#apply mask\n",
        "seq_len=x.shape[1]#no of tokens\n",
        "\n",
        "mask=torch.triu(torch.ones(seq_len,seq_len),diagonal=1).bool()\n",
        "print(\"Casual mask:\\n\",mask)\n",
        "\n",
        "attn_scores.masked_fill_(mask,-torch.inf)\n",
        "print(\"Attention scores after masking:\",attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USC73h2yETOY",
        "outputId": "4a87aa95-a6f8-4310-93b6-09b439755031"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Casual mask:\n",
            " tensor([[False,  True,  True],\n",
            "        [False, False,  True],\n",
            "        [False, False, False]])\n",
            "Attention scores after masking: tensor([[[[-318.2692,      -inf,      -inf],\n",
            "          [-294.6535,    9.5538,      -inf],\n",
            "          [ -87.5604,   -1.7744,  -12.7621]],\n",
            "\n",
            "         [[ 116.1476,      -inf,      -inf],\n",
            "          [ 178.4425,  171.2106,      -inf],\n",
            "          [  42.0843,   31.7945,   10.5541]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling by square root of key dimension and apply softmax\n",
        "torch.set_printoptions(precision=3,sci_mode=False)\n",
        "head_dim=3 #head dimension for key\n",
        "attn_weights=torch.softmax(attn_scores/head_dim**0.5,dim=-1)\n",
        "print(\"Attention weights shape:\",attn_weights.shape)\n",
        "print(\"Attention weights :\",attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd5iczamGStq",
        "outputId": "7e14ded9-e490-4cdb-8b41-60cf88d60bb6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights shape: torch.Size([1, 2, 3, 3])\n",
            "Attention weights : tensor([[[[1.000, 0.000, 0.000],\n",
            "          [0.000, 1.000, 0.000],\n",
            "          [0.000, 0.998, 0.002]],\n",
            "\n",
            "         [[1.000, 0.000, 0.000],\n",
            "          [0.985, 0.015, 0.000],\n",
            "          [0.997, 0.003, 0.000]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply dropout\n",
        "dropout=torch.nn.Dropout(0.1)\n",
        "attn_weights=dropout(attn_weights)\n",
        "print(\"Attention weight after dropout:\",attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu91QxfQHD80",
        "outputId": "2bc9f8df-6023-4eca-b221-1ea34a67f001"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weight after dropout: tensor([[[[1.111, 0.000, 0.000],\n",
            "          [0.000, 1.111, 0.000],\n",
            "          [0.000, 1.109, 0.002]],\n",
            "\n",
            "         [[1.111, 0.000, 0.000],\n",
            "          [1.094, 0.017, 0.000],\n",
            "          [1.108, 0.003, 0.000]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#find context vector\n",
        "context_vec=attn_weights@value\n",
        "print(\"Context vector:\",context_vec)\n",
        "print(\"Context vector shape:\",context_vec.shape)\n",
        "#(batch_size,no_of_heads,no_of_tokens,head_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_G9i1VqJakJ",
        "outputId": "d396b82d-67fc-48fa-f94f-b38e8d5c9ef8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector: tensor([[[[ 0.564, -3.817,  2.064],\n",
            "          [-2.124, -4.104,  2.056],\n",
            "          [-2.120, -4.099,  2.053]],\n",
            "\n",
            "         [[ 3.116,  9.936, 14.649],\n",
            "          [ 3.098,  9.814, 14.479],\n",
            "          [ 3.113,  9.915, 14.620]]]])\n",
            "Context vector shape: torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#again swap dimension 1,2\n",
        "#before swap:(batch_size,no_of_heads,no_of_tokens,head_dim)\n",
        "context_vec=context_vec.transpose(1,2)\n",
        "#after swap:(batch_size,no_of_tokens,no_of_heads,head_dim)\n",
        "print(\"Context vector after swapping:\",context_vec)\n",
        "print(\"Context vector shape after swapping:\",context_vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3MxoviwKmxx",
        "outputId": "6d8894c5-310c-47a0-ebc4-be45c57bdabd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector after swapping: tensor([[[[ 0.564, -3.817,  2.064],\n",
            "          [-2.124, -4.104,  2.056],\n",
            "          [-2.120, -4.099,  2.053]],\n",
            "\n",
            "         [[ 3.116,  9.936, 14.649],\n",
            "          [ 3.098,  9.814, 14.479],\n",
            "          [ 3.113,  9.915, 14.620]]]])\n",
            "Context vector shape after swapping: torch.Size([1, 2, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#concatenating heads\n",
        "context_vec=context_vec.reshape(batch_size,seq_len,num_heads*head_dim)\n",
        "print(\"Context vector after concatenating heads:\",context_vec)\n",
        "print(\"Context vector shape after concatenating heads:\",context_vec.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96Q0iE-YLYmA",
        "outputId": "bbd79307-6f9c-430e-c30f-6be4d1f45d6a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context vector after concatenating heads: tensor([[[ 0.564, -3.817,  2.064, -2.124, -4.104,  2.056],\n",
            "         [-2.120, -4.099,  2.053,  3.116,  9.936, 14.649],\n",
            "         [ 3.098,  9.814, 14.479,  3.113,  9.915, 14.620]]])\n",
            "Context vector shape after concatenating heads: torch.Size([1, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Multi-Head Attention Class\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,num_heads,dropout,qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert (d_out%num_heads==0),\"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_out//num_heads\n",
        "\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.out_proj=nn.Linear(d_out,d_out)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "      batch_size,num_of_tokens,d_in=x.shape\n",
        "\n",
        "      key=self.W_key(x)\n",
        "      query=self.W_query(x)\n",
        "      value=self.W_value(x)\n",
        "\n",
        "      # (batch, tokens, d_out) → (batch, tokens, num_heads, head_dim)\n",
        "      key = key.view(batch_size, num_of_tokens, self.num_heads, self.head_dim)\n",
        "      query = query.view(batch_size, num_of_tokens, self.num_heads, self.head_dim)\n",
        "      value = value.view(batch_size, num_of_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "      #arrange key,query,value based on no of heads\n",
        "      #(batch, tokens, num_heads, head_dim) → (batch, num_heads, tokens, head_dim)\n",
        "      key=key.transpose(1,2)\n",
        "      value=value.transpose(1,2)\n",
        "      query=query.transpose(1,2)\n",
        "\n",
        "      attn_scores=query@key.transpose(2,3)\n",
        "\n",
        "      mask_bool=self.mask.bool()[:num_of_tokens,:num_of_tokens]\n",
        "\n",
        "      attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "\n",
        "      attn_weights=torch.softmax(attn_scores/key.shape[-1]**0.5,dim=-1)\n",
        "\n",
        "      attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "      context_vec=attn_weights@value\n",
        "\n",
        "      #reshape and conactenating heads\n",
        "      context_vec=context_vec.transpose(1,2)\n",
        "\n",
        "      context_vec=context_vec.contiguous().view(batch_size,num_of_tokens,self.d_out)\n",
        "      context_vec=self.out_proj(context_vec)\n",
        "\n",
        "      return context_vec\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0GAnCeMwL3jE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "inputs= torch.tensor([\n",
        "    [0.12, 0.87, 0.45, 0.33, 0.91, 0.58],\n",
        "    [0.76, 0.24, 0.69, 0.11, 0.54, 0.82],\n",
        "    [0.39, 0.95, 0.18, 0.67, 0.44, 0.06]\n",
        "])\n",
        "\n",
        "batch=torch.stack((inputs,inputs),dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size,context_length,d_in=batch.shape\n",
        "d_out=6\n",
        "mha=MultiHeadAttention(d_in,d_out,context_length,2,0.0)\n",
        "context_vecs=mha(batch)\n",
        "print(context_vecs)\n",
        "print(context_vecs.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R7rtqknPtoQ",
        "outputId": "e60fb99a-c9a3-4e33-aa80-b4cfa185fb9d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.079, -0.106,  0.050,  0.058, -0.425, -0.357],\n",
            "         [ 0.067, -0.036, -0.002, -0.024, -0.330, -0.274],\n",
            "         [ 0.065, -0.079,  0.022, -0.008, -0.360, -0.334]],\n",
            "\n",
            "        [[ 0.079, -0.106,  0.050,  0.058, -0.425, -0.357],\n",
            "         [ 0.067, -0.036, -0.002, -0.024, -0.330, -0.274],\n",
            "         [ 0.065, -0.079,  0.022, -0.008, -0.360, -0.334]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([2, 3, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtM9DlkVPvKz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}